{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7012fd8",
   "metadata": {},
   "source": [
    "<a id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d144b366",
   "metadata": {},
   "source": [
    "# Spotify + Billboard Hot 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b638b9e",
   "metadata": {},
   "source": [
    "[Imports](#imports) - [Definitions](#definitions) - [Acquire](#acquire) - [Explore](#explore)  -  [Model](#model)  -  [Evaluate](#evaluate)  -  [Test](#test)  -  [Summary](#summary)  -  [Take Aways](#takeaways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e0229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from math import sqrt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90e6adb",
   "metadata": {},
   "source": [
    "<a id=\"definitions\"></a> [Imports](#imports) - [Definitions](#definitions) - [Acquire](#acquire) - [Explore](#explore)  -  [Model](#model)  -  [Evaluate](#evaluate)  -  [Test](#test)  -  [Summary](#summary)  -  [Take Aways](#takeaways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eff88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_file(filename):\n",
    "    try:\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"The file: {filename} doesn't exist\")\n",
    "        else:\n",
    "            print(\"Found File\")\n",
    "            return pd.concat([chunk for chunk in tqdm(pd.read_csv(filename, chunksize=1000), desc=f'Loading {filename}')])\n",
    "    except:\n",
    "        print(\"Didn't Work! :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a05281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "    new_names = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        level_one = re.sub('(?<!^)(?=[A-Z])', '_', column).lower()\n",
    "        level_one = re.sub(' ', '_',level_one)\n",
    "        level_one = re.sub(' _', '_',level_one)\n",
    "        level_one = re.sub('__','_',level_one)\n",
    "        new_names.append(level_one)\n",
    "    df.columns = new_names\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc7f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_dictionary(df):\n",
    "    # Printing a data dictionary using a printout of each column name\n",
    "    # formatted as a MarkDown table\n",
    "    # =================================================================\n",
    "\n",
    "    # variable to hold size of longest string in dataframe column names\n",
    "    size_longest_name = len((max((df.columns.to_list()), key = len)))\n",
    "\n",
    "    # head of markdown table\n",
    "    print(f\"| {'Name' : <{size_longest_name}} | Definition |\")\n",
    "    print(f\"| {'-'*size_longest_name} | {'-'*len('Definition')} |\")\n",
    "\n",
    "    # dataframe column content\n",
    "    for i in (df.columns.to_list()):\n",
    "        print(f\"| {i : <{size_longest_name}} | Definition |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba1e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clust(df):\n",
    "    num, cat = separate_column_type_list(df)\n",
    "    train_scaled = df[num]\n",
    "    # Create Object\n",
    "    mm_scaler = MinMaxScaler()\n",
    "    train_scaled[num] = mm_scaler.fit_transform(train_scaled[num])\n",
    "    seed = 42\n",
    "    cluster_count = 4\n",
    "\n",
    "    kmeans = KMeans(n_clusters=cluster_count,random_state=seed)\n",
    "    kmeans.fit(train_scaled)\n",
    "    df['clusters']=kmeans.predict(train_scaled)\n",
    "    sns.boxplot(data=df,x='clusters',y='alcohol',hue='quality')\n",
    "    plt.title(\"What about Clustering?\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f7249db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_2d):\n",
    "    '''\n",
    "    Function that allows other functions to accept a list as an argument\n",
    "    and not cause pesky 2d-List issues when appending a list to the \n",
    "    original argument list\n",
    "    '''\n",
    "    flattened = []\n",
    "    for item in list_2d:\n",
    "        if isinstance(item,list): flattened.extend(flatten(item))\n",
    "        else: flattened.append(item)\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0465ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    '''\n",
    "    This function take in a dataframe and splits into train validate test\n",
    "    '''\n",
    "    \n",
    "    # create train_validate and test datasets\n",
    "    train, test = train_test_split(df, train_size = 0.8, random_state = 123)\n",
    "    \n",
    "    # create train and validate datasets\n",
    "    train, validate = train_test_split(train, train_size = 0.7, random_state = 123)\n",
    "    \n",
    "    # sanity check\n",
    "    print(train.shape,validate.shape,test.shape)\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0a7482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_target(df, target):\n",
    "    '''\n",
    "    splits datasets into X,y\n",
    "    '''\n",
    "    \n",
    "    #Split into X and y\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    # sanity check\n",
    "    print(X.shape,y.shape)\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041815c",
   "metadata": {},
   "source": [
    "[Dummies](#get_dummies) <a id=\"dummy_def\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a3ef4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummies(df,dummies):\n",
    "    # keeper columns are numerical & discrete chosen\n",
    "    # for dummy creation\n",
    "    numerical = df.select_dtypes('number').columns\n",
    "    keepers = df[numerical].columns.to_list()\n",
    "    keepers.append(dummies)\n",
    "    \n",
    "    # fix list to be useable as column index\n",
    "    keepers = flatten(keepers)\n",
    "    \n",
    "    # Create dummies for non-binary categorical columns\n",
    "    df[dummies]=pd.get_dummies(df[dummies], drop_first = True)\n",
    "    \n",
    "    # drop redundant column\n",
    "    df = df.drop(df.columns.difference(keepers),axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "109aa84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_results(p, alpha, group1, group2):\n",
    "    '''\n",
    "        Test Hypothesis  using Statistics Test Output.\n",
    "        This function will take in the p-value, alpha, and a name for the 2 variables\n",
    "        you are comparing (group1 and group2) and return a string stating \n",
    "        whether or not there exists a relationship between the 2 groups. \n",
    "    '''\n",
    "    if p < alpha:\n",
    "        display(Markdown(f\"### Results:\"))\n",
    "        display(Markdown(f\"### Reject $H_0$\"))\n",
    "        display(Markdown( f'There exists some relationship between {group1} and {group2}. (p-value: {p:.4f})'))\n",
    "    \n",
    "    else:\n",
    "        display(Markdown(f\"### Results:\"))\n",
    "        display(Markdown(f\"### Failed to Reject $H_0$\"))\n",
    "        display(Markdown( f'There is not a significant relationship between {group1} and {group2}. (p-value: {p:.4f})'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "924d49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_column_type_list(df):\n",
    "    '''\n",
    "        Creates 2 lists separating continous & discrete\n",
    "        variables.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : Pandas DataFrame\n",
    "            The DataFrame from which columns will be sorted.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        continuous_columns : list\n",
    "            Columns in DataFrame with numerical values.\n",
    "        discrete_columns : list\n",
    "            Columns in DataFrame with categorical values.\n",
    "    '''\n",
    "    continuous_columns = []\n",
    "    discrete_columns = []\n",
    "    \n",
    "    for column in df.columns:\n",
    "        if (df[column].dtype == 'int' or df[column].dtype == 'float') and ('id' not in column) and (df[column].nunique()>10):\n",
    "            continuous_columns.append(column)\n",
    "        elif(df[column].dtype == 'int' or df[column].dtype == 'float') and (df[column].nunique()>11):\n",
    "            continuous_columns.append(column)\n",
    "        else:\n",
    "            discrete_columns.append(column)\n",
    "            \n",
    "    return continuous_columns, discrete_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ef1fd9",
   "metadata": {},
   "source": [
    "<a id=\"scale_def\"></a> [Scale](#scale) - [Lasso](#lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77192c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(df,mode=\"minmax\"):\n",
    "    # create a list of only continous features from input DataFrame\n",
    "    continous = df.select_dtypes('number').columns\n",
    "    \n",
    "    if mode == \"minmax\":\n",
    "        scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "        scaler.fit(df[continous])\n",
    "        df[continous] = scaler.transform(df[continous])\n",
    "        \n",
    "        return df\n",
    "\n",
    "    elif mode == \"standard\":\n",
    "        scaler = sklearn.preprocessing.StandardScaler()\n",
    "        scaler.fit(df[continous])\n",
    "        df[continous] = scaler.transform(df[continous])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(\"write new code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d6453c",
   "metadata": {},
   "source": [
    "[Modeling](#model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67549c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_vs_num_visualize(df,feature,target):\n",
    "    question = f\"Does a higher {feature} mean higher {target}?\"\n",
    "\n",
    "    #sns.scatterplot(x=df[feature], y=df[target])\n",
    "    #plt.suptitle(f\"{question}\")\n",
    "\n",
    "    #plt.show()\n",
    "    \n",
    "    sns.jointplot(data=df, x=feature, y=target,  kind='reg', height=8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c2bc258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_results(p, alpha, group1, group2):\n",
    "    '''\n",
    "        Test Hypothesis  using Statistics Test Output.\n",
    "        This function will take in the p-value, alpha, and a name for the 2 variables\n",
    "        you are comparing (group1 and group2) and return a string stating \n",
    "        whether or not there exists a relationship between the 2 groups. \n",
    "    '''\n",
    "    if p < alpha:\n",
    "        display(Markdown(f\"### Results:\"))\n",
    "        display(Markdown(f\"### Reject $H_0$\"))\n",
    "        display(Markdown( f'There exists some relationship between {group1} and {group2}. (p-value: {p:.4f})'))\n",
    "    \n",
    "    else:\n",
    "        display(Markdown(f\"### Results:\"))\n",
    "        display(Markdown(f\"### Failed to Reject $H_0$\"))\n",
    "        display(Markdown( f'There is not a significant relationship between {group1} and {group2}. (p-value: {p:.4f})'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfcbbed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_hypothesis_test(question_number,df,feature,target,alpha=.05):\n",
    "    num, cat = separate_column_type_list(df)\n",
    "    question = 'temp'\n",
    "    if (target in cat) and (feature in num):\n",
    "        # calculation\n",
    "        overall_alcohol_mean = df[feature].mean()\n",
    "        quality_sample = df[df[target] >= 7][target]\n",
    "        t, p = stats.ttest_1samp(quality_sample, overall_alcohol_mean)\n",
    "        value = t\n",
    "        p_value = p/2\n",
    "        \n",
    "        # Output variables\n",
    "        test = \"1-Sample T-Test\"\n",
    "\n",
    "        # Markdown Format Question\n",
    "        display(Markdown(f\"# Question #{question_number}:\"))\n",
    "        display(Markdown(f\"# {question}\"))\n",
    "        \n",
    "        # Visualize Question\n",
    "\n",
    "        # Markdown Formatting Metrics\n",
    "        display(Markdown(f\"### Hypothesis:\"))\n",
    "        display(Markdown(f\"$H_0$: There is no relationship between `{feature}` and `{target}`\"))\n",
    "        display(Markdown(f\"$H_A$: There is a relationship between `{feature}` and `{target}` \"))\n",
    "        display(Markdown(f\"### Statistics Test:\"))\n",
    "        display(Markdown(f\"### `{test} = {value}`\"))\n",
    "\n",
    "        # Evaluate Results\n",
    "        eval_results(p_value, alpha, feature, target)\n",
    "\n",
    "    elif (target in cat) and (feature in cat):\n",
    "        # calculations\n",
    "        observed = pd.crosstab(df[feature], df[target])\n",
    "        chi2, p, degf, expected = stats.chi2_contingency(observed)\n",
    "        value = chi2\n",
    "        p_value = p\n",
    "        \n",
    "        # Output variables\n",
    "        test = \"Chi-Square\"\n",
    "\n",
    "        # Markdown Formatting\n",
    "        display(Markdown(f\"# Question #{question_number}:\"))\n",
    "        display(Markdown(f\"# {question}\"))\n",
    "        display(Markdown(f\"### Hypothesis:\"))\n",
    "        display(Markdown(f\"$H_0$: There is no relationship between `{feature}` to `{target}`\"))\n",
    "        display(Markdown(f\"$H_A$: There is a relationship between `{feature}` and `{target}` \"))\n",
    "        display(Markdown(f\"### Statistics Test:\"))\n",
    "        display(Markdown(f\"### `{test} = {value}`\"))\n",
    "\n",
    "        eval_results(p_value, alpha, feature, target)\n",
    "\n",
    "    elif (target in num) and (feature in num):\n",
    "        # Markdown Format Question\n",
    "        question = f\"Does a higher {feature} mean higher {target}?\"\n",
    "    \n",
    "        # calculations\n",
    "        r, p = stats.pearsonr(df[feature], df[target])\n",
    "        value = r\n",
    "        p_value = p\n",
    "        \n",
    "        # Output variables\n",
    "        test = \"Pearson's R\"\n",
    "\n",
    "        # Output Question\n",
    "        display(Markdown(f\"# Question #{question_number}:\"))\n",
    "        display(Markdown(f\"# {question}\"))\n",
    "        \n",
    "        # Visualize Question\n",
    "        num_vs_num_visualize(df,feature,target)\n",
    "        \n",
    "        # Markdown Format Metrics\n",
    "        display(Markdown(f\"### Hypothesis:\"))\n",
    "        display(Markdown(f\"$H_0$: There is no relationship between `{feature}` to `{target}`\"))\n",
    "        display(Markdown(f\"$H_A$: There is a relationship between `{feature}` and `{target}` \"))\n",
    "        display(Markdown(f\"### Statistics Test:\"))\n",
    "        display(Markdown(f\"### `{test} = {value}`\"))\n",
    "    \n",
    "        # Markdown Format Evaluate Results\n",
    "        eval_results(p_value, alpha, feature, target)\n",
    "        display(Markdown(\"<hr style=\\\"border:2px solid gray\\\">\"))\n",
    "    else:\n",
    "        print(\"write code for different test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f9165",
   "metadata": {},
   "source": [
    "[Models](#modeling) <a id=\"model_def\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10012aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05a69a",
   "metadata": {},
   "source": [
    "# Acquire <a id=\"acquire\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc495d",
   "metadata": {},
   "source": [
    "[Imports](#imports) - [Definitions](#definitions) - [Acquire](#acquire) - [Explore](#explore)  -  [Model](#model)  -  [Evaluate](#evaluate)  -  [Test](#test)  -  [Summary](#summary)  -  [Take Aways](#takeaways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd42431c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hot_100.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# acquire datasets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m hot_100 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhot_100.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m spotify_popular \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msongs_normalize.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# date type column\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    663\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    664\u001b[0m     dialect,\n\u001b[1;32m    665\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    675\u001b[0m )\n\u001b[1;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hot_100.csv'"
     ]
    }
   ],
   "source": [
    "# acquire datasets\n",
    "hot_100 = pd.read_csv(\"hot_100.csv\")\n",
    "spotify_popular = pd.read_csv(\"songs_normalize.csv\")\n",
    "\n",
    "# date type column\n",
    "hot_100['chart_date']=pd.to_datetime(hot_100['chart_date'])\n",
    "\n",
    "# set date column as index\n",
    "hot_100 = hot_100.set_index(\"chart_date\").sort_index()\n",
    "hot_100_2000_2020 = hot_100.loc['2000-01-01':'2020-12-31']\n",
    "\n",
    "# create song index\n",
    "hot_100_2000_2020[\"song_index\"] = hot_100_2000_2020[\"performer\"].str.lower() + \" - \" + hot_100_2000_2020[\"song\"].str.lower()\n",
    "\n",
    "#saving extra copy of date before grouping by index\n",
    "hot_100_2000_2020['date'] = hot_100_2000_2020.index\n",
    "\n",
    "# Removing Song instance for other weeks besides the most recent\n",
    "hot_100_2000_2020_only_latest = hot_100_2000_2020.groupby('song_index', group_keys=False).apply(lambda x: x.index[np.argmax(x.index)])\n",
    "\n",
    "# converting back to dataframe\n",
    "hot_100_2000_2020_only_latest = pd.DataFrame(hot_100_2000_2020_only_latest)\n",
    "\n",
    "# swapping back the index\n",
    "hot_100_2000_2020_only_latest.columns = [\"index\"]\n",
    "\n",
    "hot_100_2000_2020_only_latest[\"unique\"]=hot_100_2000_2020_only_latest.index\n",
    "\n",
    "hot_100_2000_2020_only_latest = hot_100_2000_2020_only_latest.set_index(\"index\").sort_index()\n",
    "hot_100_2000_2020_only_latest['is_unique'] = 1\n",
    "hot_100_2000_2020_only_latest['date']=hot_100_2000_2020_only_latest.index\n",
    "hot_100_2000_2020_only_latest['song_index']=hot_100_2000_2020_only_latest['unique']\n",
    "hot_100_2000_2020_only_latest = hot_100_2000_2020_only_latest.drop(columns = ['unique'])\n",
    "merged_hot100_only_latest = hot_100_2000_2020.merge(hot_100_2000_2020_only_latest,how='left')\n",
    "merged_hot100_only_latest=merged_hot100_only_latest[merged_hot100_only_latest['is_unique']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_hot100_only_latest['1st_split']=merged_hot100_only_latest['performer'].str.split('&').str[0]\n",
    "merged_hot100_only_latest['2nd_split']=merged_hot100_only_latest['1st_split'].str.split('With').str[0]\n",
    "merged_hot100_only_latest['3rd_split']=merged_hot100_only_latest['2nd_split'].str.split('Featuring').str[0]\n",
    "merged_hot100_only_latest['4th_split']=merged_hot100_only_latest['3rd_split'].str.split(',').str[0]\n",
    "merged_hot100_only_latest['5th_split']=merged_hot100_only_latest['4th_split'].str.split(' x ').str[0]\n",
    "merged_hot100_only_latest['6th_split']=merged_hot100_only_latest['5th_split'].str.split('+').str[0]\n",
    "# final split on peformer column is aved as \"singer\" column to use in Unique ID creation\n",
    "merged_hot100_only_latest['singer']=merged_hot100_only_latest['6th_split']\n",
    "# drop old columns used as step/place-holders to isolate Artist/Singer from \"Performer\" column\n",
    "merged_hot100_only_latest = merged_hot100_only_latest.drop(columns = ['1st_split','2nd_split','3rd_split','4th_split','5th_split','6th_split'])\n",
    "# Fixing Song Title mixed characters\n",
    "merged_hot100_only_latest['song'] = merged_hot100_only_latest['song'].str.replace('[^0-9a-z - A-Z]', '')\n",
    "# Unique ID from artist & song combination\n",
    "merged_hot100_only_latest['song_index'] = merged_hot100_only_latest['singer'].str.lower() + ' - ' + merged_hot100_only_latest['song'].str.lower()\n",
    "# standardization for both datasets song_index\n",
    "merged_hot100_only_latest['song_index'] = merged_hot100_only_latest['song_index'].str.replace('[^0-9a-z-A-Z]', '')\n",
    "merged_hot100_only_latest['song_index'] = merged_hot100_only_latest['song_index'].str.replace('*', '')\n",
    "merged_hot100_only_latest['song_index'] = merged_hot100_only_latest['song_index'].str.replace('-', ' - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23399d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spotify_merge = spotify_popular.merge(merged_hot100_only_latest,how='left',on='song_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_non_nulls = new_spotify_merge.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_non_nulls.to_csv('merged_data_non_nulls.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_csv_file(\"merged_data_non_nulls.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e4d5d",
   "metadata": {},
   "source": [
    "### Cleaning Pt. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ebd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'song_x':'song'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5fdb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['is_unique','singer','test_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['performer_x', 'song_index', 'test_x', 'song_y', 'performer_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8985d",
   "metadata": {},
   "source": [
    "# Goals <a id=\"goals\"></a>\n",
    "* Discover drivers of charting on the Billboard 100 in the Spotify Data\n",
    "* Use significant drivers to develop a machine learning model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c767420",
   "metadata": {},
   "source": [
    "# Acquire\n",
    "\n",
    "* Data was acquired from Kaggle & Github\n",
    "* Left merged Billboard dataset using unique key combined from features\n",
    "* Spotify dataset contained 2000 samples and 20 features\n",
    "* Billboard Hot 100 dataset contained 336_295 samples and 12 features\n",
    "* Each row represents a song consindered a hit\n",
    "* Each column represents a feature of those songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b5adb",
   "metadata": {},
   "source": [
    "# Prepare\n",
    "\n",
    "**Prepare Actions:**\n",
    "* Removed columns that did not contain useful information\n",
    "* Removed duplicate columns due to larger SQL Query\n",
    "* Checked for nulls in the data (there were none)\n",
    "* Checked that column data types were appropriate\n",
    "* Fixed incorrect dtatypes\n",
    "* Added additional features to investigate:\n",
    "    * Total Add-on Count\n",
    "* Encoded categorical bi-variate\n",
    "* Created dummy variables for the multi-variate features\n",
    "* Split data into train, validate and test (approx. 64/20/16), stratifying on 'churn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac0f0da",
   "metadata": {},
   "source": [
    "# Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc0865",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478af0f7",
   "metadata": {},
   "source": [
    "| Name             | Definition |\n",
    "| ---------------- | ---------- |\n",
    "| duration_ms      | Definition |\n",
    "| energy           | Definition |\n",
    "| loudness         | Definition |\n",
    "| valence          | Definition |\n",
    "| acousticness     | Definition |\n",
    "| year             | Definition |\n",
    "| explicit         | Definition |\n",
    "| instrumentalness | Definition |\n",
    "| mode             | Definition |\n",
    "| speechiness      | Definition |\n",
    "| explicit         | Definition |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f1ee4",
   "metadata": {},
   "source": [
    "# Explore <a id=\"explore\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526cfa8d",
   "metadata": {},
   "source": [
    "[Imports](#imports) - [Definitions](#definitions) - [Acquire](#acquire) - [Explore](#explore)  -  [Model](#model)  -  [Evaluate](#evaluate)  -  [Test](#test)  -  [Summary](#summary)  -  [Take Aways](#takeaways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662cc114",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'time_on_chart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755066e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_columns_list = df.select_dtypes('number').columns\n",
    "for num,feature in enumerate(num_columns_list):\n",
    "    question_hypothesis_test((num+1),df,feature,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d70330",
   "metadata": {},
   "source": [
    "### Exploration Summary\n",
    "* \"Senior Citizen Status\" was found to be a driver of \"churn\"\n",
    "* \"Tech Support\" was found to be a driver of \"churn\" \n",
    "* Added feature \"Total Addons\" was found to be a driver of \"churn\" \n",
    "\n",
    "### Features I am moving to modeling With\n",
    "* \"Senior Citizen Status\" (small difference in churn percentage, but relationship to churn is statistically significant)\n",
    "* \"Tech Support\" (small difference in churn percentage, but relationship to churn is statistically significant)\n",
    "* \"Total Addons\" (moderate difference in churn percentage, and dependance is statistically significant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9f52a",
   "metadata": {},
   "source": [
    "# Model <a id=\"model\"></a>\n",
    "* I used RMSE as my evaluation metric\n",
    "</br>\n",
    "\n",
    "* churn makes up 26.54% of the data \n",
    "* by guessing not churn for every customer one could achieve an accuracy of 73.46%\n",
    "* 73.46% will be the baseline accuracy I use for this project\n",
    "</br>\n",
    "\n",
    "* I will be evaluating models developed using three different model types and various hyperparameter configurations \n",
    "* Models will be evaluated on train and validate data\n",
    "* The model that performs the best will then be evaluated on test data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d5c73c",
   "metadata": {},
   "source": [
    "[Imports](#imports) - [Definitions](#definitions) - [Acquire](#acquire) - [Explore](#explore)  -  [Model](#model)  -  [Evaluate](#evaluate)  -  [Test](#test)  -  [Summary](#summary)  -  [Take Aways](#takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cea9d4",
   "metadata": {},
   "source": [
    "### Pre-Process Obvious Continous Data Target Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['chart_position', 'instance','popularity',\n",
    "       'consecutive_weeks', 'previous_week', 'peak_position',\n",
    "       'worst_position'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ed804",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b345f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eedf3fa",
   "metadata": {},
   "source": [
    "### Isolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = isolate_target(train,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff38dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate, y_validate = isolate_target(val,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = isolate_target(test,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e900f",
   "metadata": {},
   "source": [
    "### Scale <a id=\"scale\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039a2bb",
   "metadata": {},
   "source": [
    "<a id=\"definitions\"></a> [Definitions](#scale_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11446a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scale_data(X_train)\n",
    "X_validate_scaled = scale_data(X_validate)\n",
    "X_test_scaled = scale_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f77c0",
   "metadata": {},
   "source": [
    "### Dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852eec4",
   "metadata": {},
   "source": [
    "[Definitions](#dummy_def) <a id=\"get_dummies\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7748f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dummies = ['explicit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = dummies(X_train_scaled,to_dummies)\n",
    "X_validate_scaled = dummies(X_validate_scaled,to_dummies)\n",
    "X_test_scaled = dummies(X_test_scaled,to_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d43ca",
   "metadata": {},
   "source": [
    "### Models <a id=\"modeling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7414d8b",
   "metadata": {},
   "source": [
    "[Definitions](#model_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99030bb",
   "metadata": {},
   "source": [
    "#### Step 1: Feature Selection Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e968120",
   "metadata": {},
   "source": [
    "#### Method 1 - F-Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters: f_regression stats test, give me 8 features\n",
    "f_selector = SelectKBest(f_regression, k=4)\n",
    "\n",
    "# find the top 8 X's correlated with y\n",
    "f_selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "# boolean mask of whether the column was selected or not. \n",
    "feature_mask = f_selector.get_support()\n",
    "\n",
    "# get list of top K features. \n",
    "f_feature = X_train_scaled.iloc[:,feature_mask].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b5d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a924dc9",
   "metadata": {},
   "source": [
    "#### Method 2: RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ML algorithm\n",
    "lm = LinearRegression()\n",
    "\n",
    "# create the rfe object, indicating the ML object (lm) and the number of features I want to end up with. \n",
    "rfe = RFE(lm, n_features_to_select=4)\n",
    "\n",
    "# fit the data using RFE\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "\n",
    "# get the mask of the columns selected\n",
    "feature_mask = rfe.support_\n",
    "\n",
    "# get list of the column names. \n",
    "rfe_feature = X_train_scaled.iloc[:,feature_mask].columns.tolist()\n",
    "rfe_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0baf796",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ranks = rfe.ranking_\n",
    "\n",
    "# get the variable names\n",
    "var_names = X_train_scaled.columns.tolist()\n",
    "\n",
    "# combine ranks and names into a df for clean viewing\n",
    "rfe_ranks_df = pd.DataFrame({'Var': var_names, 'Rank': var_ranks})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eacbd59",
   "metadata": {},
   "source": [
    "#### Step 2: Keeping Best Features From Both Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3b677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping Rank >= 7\n",
    "modeling_features = rfe_ranks_df[rfe_ranks_df['Rank']<=7].sort_values('Rank')\n",
    "modeling_features = [modeling_features.iloc[i][0] for i in range(10)]\n",
    "modeling_features.append(to_dummies)\n",
    "modeling_features = flatten(modeling_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing uneeded features after RFE\n",
    "X_train_scaled = X_train_scaled[modeling_features]\n",
    "X_validate_scaled = X_validate_scaled[modeling_features]\n",
    "X_test_scaled = X_test_scaled[modeling_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3747b53",
   "metadata": {},
   "source": [
    "#### Step 3: Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99eaf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Model\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train_scaled, y_train)\n",
    "preds['linear'] = lm.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1f6e6",
   "metadata": {},
   "source": [
    "<a id=\"lasso\"></a> [Definitions](#scale_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a55dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific Scaling for Lasso Lars\n",
    "X_train_lasso_scale = scale_data(X_train,mode=\"standard\")\n",
    "X_train_lasso_scale = dummies(X_train_lasso_scale,to_dummies)\n",
    "\n",
    "# Lasso Lars Model\n",
    "lars = LassoLars(alpha=1.0)\n",
    "lars.fit(X_train_lasso_scale, y_train)\n",
    "preds['lasso_lars'] = lars.predict(X_train_lasso_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19272c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweedie Model\n",
    "glm = TweedieRegressor(power=1, alpha=0)\n",
    "glm.fit(X_train_scaled, y_train)\n",
    "preds['Tweedie'] = glm.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42fd97",
   "metadata": {},
   "source": [
    "### Comparing Models\n",
    "\n",
    "* Out of the Random Forest, KNN, and Logistic Regression models Random Forest performed higher than baseline on train and validate\n",
    "* The Logistic Regression model performed slightly better on validate data but was worse than baseline on either one.\n",
    "\n",
    "* I have chosen to move forward to the test set with the Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec39136e",
   "metadata": {},
   "source": [
    "### Evaluate <a id=\"evaluate\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e1059a",
   "metadata": {},
   "source": [
    "[Imports](#imports) - [Definitions](#definitions) - [Acquire](#acquire) - [Explore](#explore)  -  [Model](#model)  -  [Evaluate](#evaluate)  -  [Test](#test)  -  [Summary](#summary)  -  [Take Aways](#takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede3929",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be1237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "preds['actual'] = y_train\n",
    "preds['baseline'] = y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e389f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline RMSE Metric\n",
    "print(f\"Baseline RMSE = {(sqrt(mean_squared_error(preds['actual'], preds['baseline'])))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba40ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear RMSE Metric\n",
    "sqrt(mean_squared_error(preds['actual'], preds['linear']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d7070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  RMSE Metric\n",
    "sqrt(mean_squared_error(preds['actual'], preds['lasso_lars']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc36489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  RMSE Metric\n",
    "sqrt(mean_squared_error(preds['actual'], preds['Tweedie']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b65d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4ff75e3",
   "metadata": {},
   "source": [
    "### Test <a id=\"test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae97e3",
   "metadata": {},
   "source": [
    "[Imports](#imports) - [Definitions](#definitions) - [Acquire](#acquire) - [Explore](#explore)  -  [Model](#model)  -  [Evaluate](#evaluate)  -  [Test](#test)  -  [Summary](#summary)  -  [Take Aways](#takeaways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ad8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00ebb9d2",
   "metadata": {},
   "source": [
    "### Modeling Summary\n",
    "\n",
    "* Out of the Random Forest, KNN, and Logistic Regression models Random Forest performed higher than baseline on train and validate\n",
    "* The Logistic Regression model performed slightly better on validate data but was worse than baseline on either one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dfb52c",
   "metadata": {},
   "source": [
    "# Conclusion <a id=\"conclusion\"></a>\n",
    "### Exploration\n",
    "\n",
    "* Total churn = 1869 out of 7043 customers.\n",
    "* 26.5% Churned\n",
    "* 73.5% Not Churned\n",
    "\n",
    "### Modeling\n",
    "\n",
    "* Out of the Random Forest, KNN, and Logistic Regression models Random Forest performed higher than baseline on train and validate\n",
    "* The Logistic Regression model performed slightly better on validate data but was worse than baseline on either one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a7b6c6",
   "metadata": {},
   "source": [
    "[Imports](#imports) - [Definitions](#definitions) - [Acquire](#acquire) - [Explore](#explore)  -  [Model](#model)  -  [Evaluate](#evaluate)  -  [Test](#test)  -  [Summary](#summary)  -  [Take Aways](#takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aea1df",
   "metadata": {},
   "source": [
    "# Takeaways <a id=\"takeaways\"></a>\n",
    "[Imports](#imports) - [Definitions](#definitions) - [Acquire](#acquire) - [Explore](#explore)  -  [Model](#model)  -  [Evaluate](#evaluate)  -  [Test](#test)  -  [Summary](#summary)  -  [Take Aways](#takeaways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d108f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d945a38",
   "metadata": {},
   "source": [
    "# Next Steps <a id=\"takeaways\"></a>\n",
    "* For loop chi square all categorical features against churn, and sort each feature by accuracy\n",
    "* Explore further numerical features\n",
    "* Look for other ways to calculate \"Total Charges\" Such as standard deviation from the mean and compare against churn\n",
    "\n",
    "[Imports](#imports) - [Definitions](#definitions) - [Acquire](#acquire) - [Explore](#explore)  -  [Model](#model)  -  [Evaluate](#evaluate)  -  [Test](#test)  -  [Summary](#summary)  -  [Take Aways](#takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f4b900",
   "metadata": {},
   "source": [
    "[Imports](#imports) - [Definitions](#definitions) - [Acquire](#acquire) - [Explore](#explore)  -  [Model](#model)  -  [Evaluate](#evaluate)  -  [Test](#test)  -  [Summary](#summary)  -  [Take Aways](#takeaways)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
